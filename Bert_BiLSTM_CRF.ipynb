{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "import torch\n",
    "import os\n",
    "import datetime\n",
    "import unicodedata\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset,DataLoader\n",
    "import torch.nn.functional as F\n",
    "from pytorch_pretrained_bert import BertModel\n",
    "from evaluating import Metrics\n",
    "from torch.autograd import Variable\n",
    "import ipdb\n",
    "import time\n",
    "import json\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    base_epoch = 1\n",
    "    batch_size = 16\n",
    "    max_length = 110\n",
    "    require_improvement = 1000  \n",
    "    bert_embedding = 768\n",
    "    rnn_hidden = 100\n",
    "    tagset_size = 10\n",
    "    bert_path = './data/bert-base-chinese'\n",
    "    rnn_layers = 1\n",
    "    dropout1 = 0.5\n",
    "    dropout_ratio = 0.5\n",
    "    # learning_rate = 1e-5\n",
    "    lr = 0.0001\n",
    "    lr_decay = 0.00001\n",
    "    weight_decay = 0.00005\n",
    "    use_cuda = True\n",
    "    optim = 'Adam'\n",
    "    save_path = './models/Bert_BiLSTM_CRF.ckpt'\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    data_path = 'data/all_data.txt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_corpus(data_path,make_word2id =True):\n",
    "    word_lists = []\n",
    "    tag_lists = []\n",
    "    with open(data_path,'r',encoding='utf-8') as f:\n",
    "        word_list = []\n",
    "        tag_list = []\n",
    "        \n",
    "        for line in f:\n",
    "            if line !='\\n':\n",
    "                line = line.strip('\\n').split()\n",
    "                if len(line) < 2:\n",
    "                    continue\n",
    "                word,tag = line[0],line[1]\n",
    "\n",
    "                word_list.append(word)\n",
    "                tag_list.append(tag)\n",
    "            else:\n",
    "                word_lists.append(word_list)\n",
    "                tag_lists.append(tag_list)\n",
    "                word_list = []\n",
    "                tag_list = []\n",
    "    def build_map(lists):\n",
    "        maps = {}\n",
    "        for sent in lists:\n",
    "            for word in sent:\n",
    "                if word not in maps:\n",
    "                    maps[word]=len(maps)\n",
    "        return maps\n",
    "\n",
    "    if make_word2id:\n",
    "        word2id = build_map(word_lists)\n",
    "        tag2id = build_map(tag_lists)\n",
    "        return word_lists,tag_lists,word2id,tag2id\n",
    "    else:\n",
    "        return word_lists,tag_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_data(word_lists,tag_lists,sent_length):\n",
    "    for i in range(len(word_lists)):\n",
    "        word_lists[i] = ['[CLS]'] + word_lists[i] + ['[SEP]']\n",
    "        word_lists[i] = word_lists[i] + (sent_length - len(word_lists[i]))*['<pad>']\n",
    "\n",
    "        tag_lists[i] = ['<start>'] + tag_lists[i] + ['<eos>']\n",
    "        tag_lists[i] = tag_lists[i] + (sent_length - len(tag_lists[i]))*['<pad>']\n",
    "    return word_lists,tag_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_maps(word2id, tag2id):\n",
    "    word2id['<unk>'] = len(word2id)\n",
    "    word2id['<pad>'] = len(word2id)\n",
    "    word2id['[CLS]'] = len(word2id)\n",
    "    word2id['[SEP]'] = len(word2id)\n",
    "\n",
    "    tag2id['<pad>'] = len(tag2id)\n",
    "    tag2id['<start>'] = len(tag2id)\n",
    "    tag2id['<eos>'] = len(tag2id)\n",
    "    return word2id, tag2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM模型 工具函数\n",
    "def tensorized(data,maps):\n",
    "    UNK = maps.get('<unk>')\n",
    "    PAD = maps.get('<pad>')\n",
    "\n",
    "    max_len = len(data[0])\n",
    "    sent_length = len(data)\n",
    "    data_tensor = torch.ones(sent_length,max_len).long()*PAD\n",
    "    for i,sen in enumerate(data):\n",
    "        for j,word in enumerate(sen):\n",
    "            data_tensor[i][j] = maps.get(word,UNK)\n",
    "    return data_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mask_tensor(data):\n",
    "    max_len = len(data[0])\n",
    "    sent_length = len(data)\n",
    "\n",
    "    mask_tensor = torch.ones(sent_length,max_len).long()\n",
    "    for i,sen in enumerate(data):\n",
    "        for j, word in enumerate(sen):\n",
    "            if word == '<pad>':\n",
    "                mask_tensor[i][j] = 0\n",
    "    return mask_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT_LSTM_CRF(nn.Module):\n",
    "    \"\"\"\n",
    "    bert_lstm_crf model\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super(BERT_LSTM_CRF, self).__init__()\n",
    "        self.embedding_dim = config.bert_embedding\n",
    "        self.hidden_dim = config.rnn_hidden\n",
    "        self.bert = BertModel.from_pretrained(config.bert_path)\n",
    "        self.lstm = nn.LSTM(config.bert_embedding, config.rnn_hidden,\n",
    "                            num_layers=config.rnn_layers, bidirectional=True, dropout=config.dropout_ratio, batch_first=True)\n",
    "        self.rnn_layers = config.rnn_layers\n",
    "        self.dropout1 = nn.Dropout(p=config.dropout1)\n",
    "        self.crf = CRF(target_size=config.tagset_size, average_batch=True, use_cuda=config.use_cuda)\n",
    "        self.liner = nn.Linear(config.rnn_hidden*2, config.tagset_size + 2)\n",
    "        self.tagset_size = config.tagset_size\n",
    "\n",
    "    def rand_init_hidden(self, batch_size):\n",
    "        \"\"\"\n",
    "        random initialize hidden variable\n",
    "        \"\"\"\n",
    "        hidden_state = torch.randn(2 * self.rnn_layers, batch_size, self.hidden_dim)\n",
    "        cell_state = torch.randn(2 * self.rnn_layers, batch_size, self.hidden_dim)\n",
    "        return hidden_state,cell_state\n",
    "        \n",
    "    def forward(self, sentence, attention_mask=None):\n",
    "        '''\n",
    "        args:\n",
    "            sentence (word_seq_len, batch_size) : word-level representation of sentence\n",
    "            hidden: initial hidden state\n",
    "\n",
    "        return:\n",
    "            crf output (word_seq_len, batch_size, tag_size, tag_size), hidden\n",
    "        '''\n",
    "        batch_size = sentence.size(0)\n",
    "        seq_length = sentence.size(1)\n",
    "        embeds, _ = self.bert(sentence, attention_mask=attention_mask, output_all_encoded_layers=False)\n",
    "        # hidden = self.rand_init_hidden(batch_size)\n",
    "        # if embeds.is_cuda:\n",
    "        #     hidden = (i.cuda() for i in hidden)\n",
    "        lstm_out, hidden = self.lstm(embeds)\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim*2)\n",
    "        d_lstm_out = self.dropout1(lstm_out)\n",
    "        l_out = self.liner(d_lstm_out)\n",
    "        lstm_feats = l_out.contiguous().view(batch_size, seq_length, -1)\n",
    "        return lstm_feats\n",
    "\n",
    "    def loss(self, feats, mask, tags):\n",
    "        \"\"\"\n",
    "        feats: size=(batch_size, seq_len, tag_size)\n",
    "            mask: size=(batch_size, seq_len)\n",
    "            tags: size=(batch_size, seq_len)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        loss_value = self.crf.neg_log_likelihood_loss(feats, mask, tags)\n",
    "        batch_size = feats.size(0)\n",
    "        loss_value /= float(batch_size)\n",
    "        return loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_sum_exp(vec, m_size):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        vec: size=(batch_size, vanishing_dim, hidden_dim)\n",
    "        m_size: hidden_dim\n",
    "\n",
    "    Returns:\n",
    "        size=(batch_size, hidden_dim)\n",
    "    \"\"\"\n",
    "    _, idx = torch.max(vec, 1)  # B * 1 * M\n",
    "    max_score = torch.gather(vec, 1, idx.view(-1, 1, m_size)).view(-1, 1, m_size)  # B * M\n",
    "    return max_score.view(-1, m_size) + torch.log(torch.sum(\n",
    "        torch.exp(vec - max_score.expand_as(vec)), 1)).view(-1, m_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CRF(nn.Module):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            target_size: int, target size\n",
    "            use_cuda: bool, 是否使用gpu, default is True\n",
    "            average_batch: bool, loss是否作平均, default is True\n",
    "        \"\"\"\n",
    "        super(CRF, self).__init__()\n",
    "        for k in kwargs:\n",
    "            self.__setattr__(k, kwargs[k])\n",
    "        self.START_TAG_IDX, self.END_TAG_IDX = -2, -1\n",
    "        init_transitions = torch.zeros(self.target_size+2, self.target_size+2)\n",
    "        init_transitions[:, self.START_TAG_IDX] = -1000.\n",
    "        init_transitions[self.END_TAG_IDX, :] = -1000.\n",
    "        if self.use_cuda:\n",
    "            init_transitions = init_transitions.cuda()\n",
    "        self.transitions = nn.Parameter(init_transitions)\n",
    "\n",
    "    def _forward_alg(self, feats, mask=None):\n",
    "        \"\"\"\n",
    "        Do the forward algorithm to compute the partition function (batched).\n",
    "\n",
    "        Args:\n",
    "            feats: size=(batch_size, seq_len, self.target_size+2)\n",
    "            mask: size=(batch_size, seq_len)\n",
    "\n",
    "        Returns:\n",
    "            xxx\n",
    "        \"\"\"\n",
    "        batch_size = feats.size(0)\n",
    "        seq_len = feats.size(1)\n",
    "        tag_size = feats.size(-1)\n",
    "\n",
    "        mask = mask.transpose(1, 0).contiguous()\n",
    "        ins_num = batch_size * seq_len\n",
    "        feats = feats.transpose(1, 0).contiguous().view(\n",
    "            ins_num, 1, tag_size).expand(ins_num, tag_size, tag_size)\n",
    "\n",
    "        scores = feats + self.transitions.view(\n",
    "            1, tag_size, tag_size).expand(ins_num, tag_size, tag_size)\n",
    "        scores = scores.view(seq_len, batch_size, tag_size, tag_size)\n",
    "        seq_iter = enumerate(scores)\n",
    "        try:\n",
    "            _, inivalues = seq_iter.__next__()\n",
    "        except:\n",
    "            _, inivalues = seq_iter.next()\n",
    "\n",
    "        partition = inivalues[:, self.START_TAG_IDX, :].clone().view(batch_size, tag_size, 1)\n",
    "        for idx, cur_values in seq_iter:\n",
    "            cur_values = cur_values + partition.contiguous().view(\n",
    "                batch_size, tag_size, 1).expand(batch_size, tag_size, tag_size)\n",
    "            cur_partition = log_sum_exp(cur_values, tag_size)\n",
    "            mask_idx = mask[idx, :].view(batch_size, 1).expand(batch_size, tag_size)\n",
    "            masked_cur_partition = cur_partition.masked_select(mask_idx.byte())\n",
    "            if masked_cur_partition.dim() != 0:\n",
    "                mask_idx = mask_idx.contiguous().view(batch_size, tag_size, 1)\n",
    "                partition.masked_scatter_(mask_idx.byte(), masked_cur_partition)\n",
    "        cur_values = self.transitions.view(1, tag_size, tag_size).expand(\n",
    "            batch_size, tag_size, tag_size) + partition.contiguous().view(\n",
    "                batch_size, tag_size, 1).expand(batch_size, tag_size, tag_size)\n",
    "        cur_partition = log_sum_exp(cur_values, tag_size)\n",
    "        final_partition = cur_partition[:, self.END_TAG_IDX]\n",
    "        return final_partition.sum(), scores\n",
    "\n",
    "    def _viterbi_decode(self, feats, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            feats: size=(batch_size, seq_len, self.target_size+2)\n",
    "            mask: size=(batch_size, seq_len)\n",
    "\n",
    "        Returns:\n",
    "            decode_idx: (batch_size, seq_len), viterbi decode结果\n",
    "            path_score: size=(batch_size, 1), 每个句子的得分\n",
    "        \"\"\"\n",
    "        batch_size = feats.size(0)\n",
    "        seq_len = feats.size(1)\n",
    "        tag_size = feats.size(-1)\n",
    "\n",
    "        length_mask = torch.sum(mask, dim=1).view(batch_size, 1).long()\n",
    "        mask = mask.transpose(1, 0).contiguous()\n",
    "        ins_num = seq_len * batch_size\n",
    "        feats = feats.transpose(1, 0).contiguous().view(\n",
    "            ins_num, 1, tag_size).expand(ins_num, tag_size, tag_size)\n",
    "\n",
    "        scores = feats + self.transitions.view(\n",
    "            1, tag_size, tag_size).expand(ins_num, tag_size, tag_size)\n",
    "        scores = scores.view(seq_len, batch_size, tag_size, tag_size)\n",
    "\n",
    "        seq_iter = enumerate(scores)\n",
    "        # record the position of the best score\n",
    "        back_points = list()\n",
    "        partition_history = list()\n",
    "        mask = (1 - mask.long()).byte()\n",
    "        try:\n",
    "            _, inivalues = seq_iter.__next__()\n",
    "        except:\n",
    "            _, inivalues = seq_iter.next()\n",
    "        partition = inivalues[:, self.START_TAG_IDX, :].clone().view(batch_size, tag_size, 1)\n",
    "        partition_history.append(partition)\n",
    "\n",
    "        for idx, cur_values in seq_iter:\n",
    "            cur_values = cur_values + partition.contiguous().view(\n",
    "                batch_size, tag_size, 1).expand(batch_size, tag_size, tag_size)\n",
    "            partition, cur_bp = torch.max(cur_values, 1)\n",
    "            partition_history.append(partition.unsqueeze(-1))\n",
    "\n",
    "            cur_bp.masked_fill_(mask[idx].view(batch_size, 1).expand(batch_size, tag_size), 0)\n",
    "            back_points.append(cur_bp)\n",
    "\n",
    "        partition_history = torch.cat(partition_history).view(\n",
    "            seq_len, batch_size, -1).transpose(1, 0).contiguous()\n",
    "\n",
    "        last_position = length_mask.view(batch_size, 1, 1).expand(batch_size, 1, tag_size) - 1\n",
    "        last_partition = torch.gather(\n",
    "            partition_history, 1, last_position).view(batch_size, tag_size, 1)\n",
    "\n",
    "        last_values = last_partition.expand(batch_size, tag_size, tag_size) + \\\n",
    "            self.transitions.view(1, tag_size, tag_size).expand(batch_size, tag_size, tag_size)\n",
    "        _, last_bp = torch.max(last_values, 1)\n",
    "        pad_zero = Variable(torch.zeros(batch_size, tag_size)).long()\n",
    "        if self.use_cuda:\n",
    "            pad_zero = pad_zero.cuda()\n",
    "        back_points.append(pad_zero)\n",
    "        back_points = torch.cat(back_points).view(seq_len, batch_size, tag_size)\n",
    "\n",
    "        pointer = last_bp[:, self.END_TAG_IDX]\n",
    "        insert_last = pointer.contiguous().view(batch_size, 1, 1).expand(batch_size, 1, tag_size)\n",
    "        back_points = back_points.transpose(1, 0).contiguous()\n",
    "\n",
    "        back_points.scatter_(1, last_position, insert_last)\n",
    "\n",
    "        back_points = back_points.transpose(1, 0).contiguous()\n",
    "\n",
    "        decode_idx = Variable(torch.LongTensor(seq_len, batch_size))\n",
    "        if self.use_cuda:\n",
    "            decode_idx = decode_idx.cuda()\n",
    "        decode_idx[-1] = pointer.data\n",
    "        for idx in range(len(back_points)-2, -1, -1):\n",
    "            pointer = torch.gather(back_points[idx], 1, pointer.contiguous().view(batch_size, 1))\n",
    "            decode_idx[idx] = pointer.view(-1).data\n",
    "        path_score = None\n",
    "        decode_idx = decode_idx.transpose(1, 0)\n",
    "        return path_score, decode_idx\n",
    "\n",
    "    def forward(self, feats, mask=None):\n",
    "        path_score, best_path = self._viterbi_decode(feats, mask)\n",
    "        return path_score, best_path\n",
    "\n",
    "    def _score_sentence(self, scores, mask, tags):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            scores: size=(seq_len, batch_size, tag_size, tag_size)\n",
    "            mask: size=(batch_size, seq_len)\n",
    "            tags: size=(batch_size, seq_len)\n",
    "\n",
    "        Returns:\n",
    "            score:\n",
    "        \"\"\"\n",
    "        batch_size = scores.size(1)\n",
    "        seq_len = scores.size(0)\n",
    "        tag_size = scores.size(-1)\n",
    "\n",
    "        new_tags = Variable(torch.LongTensor(batch_size, seq_len))\n",
    "        if self.use_cuda:\n",
    "            new_tags = new_tags.cuda()\n",
    "        for idx in range(seq_len):\n",
    "            if idx == 0:\n",
    "                new_tags[:, 0] = (tag_size - 2) * tag_size + tags[:, 0]\n",
    "            else:\n",
    "                new_tags[:, idx] = tags[:, idx-1] * tag_size + tags[:, idx]\n",
    "\n",
    "        end_transition = self.transitions[:, self.END_TAG_IDX].contiguous().view(\n",
    "            1, tag_size).expand(batch_size, tag_size)\n",
    "        length_mask = torch.sum(mask, dim=1).view(batch_size, 1).long()\n",
    "        end_ids = torch.gather(tags, 1, length_mask-1)\n",
    "\n",
    "        end_energy = torch.gather(end_transition, 1, end_ids)\n",
    "\n",
    "        new_tags = new_tags.transpose(1, 0).contiguous().view(seq_len, batch_size, 1)\n",
    "        tg_energy = torch.gather(scores.view(seq_len, batch_size, -1), 2, new_tags).view(\n",
    "            seq_len, batch_size)\n",
    "        tg_energy = tg_energy.masked_select(mask.transpose(1, 0))\n",
    "\n",
    "        gold_score = tg_energy.sum() + end_energy.sum()\n",
    "\n",
    "        return gold_score\n",
    "\n",
    "    def neg_log_likelihood_loss(self, feats, mask, tags):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            feats: size=(batch_size, seq_len, tag_size)\n",
    "            mask: size=(batch_size, seq_len)\n",
    "            tags: size=(batch_size, seq_len)\n",
    "        \"\"\"\n",
    "        batch_size = feats.size(0)\n",
    "        mask = mask.byte()\n",
    "        forward_score, scores = self._forward_alg(feats, mask)\n",
    "        gold_score = self._score_sentence(scores, mask, tags)\n",
    "        if self.average_batch:\n",
    "            return (forward_score - gold_score) / batch_size\n",
    "        return forward_score - gold_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config\n",
    "#读取数据，划分训练集，验证集测试集\n",
    "word_lists,tag_lists,word2id,tag2id = build_corpus(config.data_path,make_word2id =True)\n",
    "train_word_lists,train_tag_lists = word_lists[:800000],tag_lists[:800000]\n",
    "dev_word_lists,dev_tag_lists = word_lists[800000:900000],tag_lists[800000:900000]\n",
    "test_word_lists,test_tag_lists = word_lists[900000:],tag_lists[900000:]\n",
    "\n",
    "#先把word2id,tag2id保存\n",
    "bert_word2id,bert_tag2id = extend_maps(word2id, tag2id)\n",
    "json.dump(bert_word2id,open('data/bert_word2id.txt','w'))\n",
    "json.dump(bert_tag2id,open('data/bert_tag2id.txt','w'))\n",
    "\n",
    "#在句子首尾分别加上[cls],[sep],并把每条数据用0补全至max_length\n",
    "train_word_lists,train_tag_lists = pad_data(train_word_lists,train_tag_lists,Config.max_length)\n",
    "dev_word_lists,dev_tag_lists = pad_data(dev_word_lists,dev_tag_lists,Config.max_length)\n",
    "test_word_lists,test_tag_lists = pad_data(test_word_lists,test_tag_lists,Config.max_length)\n",
    "\n",
    "#将每个字，标签转化为索引，并将数据格式转化为tensor\n",
    "train_data_tensor,train_tag_tensor = tensorized(train_word_lists,word2id),tensorized(train_tag_lists,tag2id)\n",
    "dev_data_tensor,dev_tag_tensor = tensorized(dev_word_lists,word2id),tensorized(dev_tag_lists,tag2id)\n",
    "test_data_tensor,test_tag_tensor = tensorized(test_word_lists,word2id),tensorized(test_tag_lists,tag2id)\n",
    "\n",
    "#获取mask_tensor,例如：word_list:['我','在','成','都','[<pad>]','[<pad>]'],mask_tensor:tensor([1,1,1,1,0,0])\n",
    "train_mask_tensor = get_mask_tensor(train_word_lists)\n",
    "dev_mask_tensor = get_mask_tensor(dev_word_lists)\n",
    "test_mask_tensor = get_mask_tensor(test_word_lists)\n",
    "\n",
    "#把数据制作成data_loader\n",
    "train_dataset = TensorDataset(train_data_tensor, train_mask_tensor, train_tag_tensor)\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=config.batch_size)\n",
    "\n",
    "dev_dataset = TensorDataset(dev_data_tensor, dev_mask_tensor, dev_tag_tensor)\n",
    "dev_loader = DataLoader(dev_dataset, shuffle=True, batch_size=config.batch_size)\n",
    "\n",
    "test_dataset = TensorDataset(test_data_tensor, test_mask_tensor, test_tag_tensor)\n",
    "test_loader = DataLoader(test_dataset, shuffle=True, batch_size=config.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time_dif(start_time):\n",
    "    \"\"\"获取已使用时间\"\"\"\n",
    "    end_time = time.time()\n",
    "    time_dif = end_time - start_time\n",
    "    return timedelta(seconds=int(round(time_dif)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "step: 100 | epoch: 1 | train_loss: 1.623 | dev_loss:1.552 | improve:* | time:0:26:19\n",
      "step: 200 | epoch: 1 | train_loss: 0.8484 | dev_loss:0.8132 | improve:* | time:0:51:57\n",
      "step: 300 | epoch: 1 | train_loss: 0.3706 | dev_loss:0.564 | improve:* | time:1:17:43\n",
      "step: 400 | epoch: 1 | train_loss: 0.4235 | dev_loss:0.4644 | improve:* | time:1:43:43\n",
      "step: 500 | epoch: 1 | train_loss: 0.433 | dev_loss:0.3855 | improve:* | time:2:10:23\n",
      "step: 600 | epoch: 1 | train_loss: 0.3956 | dev_loss:0.3631 | improve:* | time:2:35:57\n",
      "step: 700 | epoch: 1 | train_loss: 0.362 | dev_loss:0.3216 | improve:* | time:3:01:35\n",
      "step: 800 | epoch: 1 | train_loss: 0.4344 | dev_loss:0.3103 | improve:* | time:3:27:13\n",
      "step: 900 | epoch: 1 | train_loss: 0.2897 | dev_loss:0.2722 | improve:* | time:3:52:46\n",
      "step: 1000 | epoch: 1 | train_loss: 0.3525 | dev_loss:0.2689 | improve:* | time:4:18:18\n",
      "step: 1100 | epoch: 1 | train_loss: 0.209 | dev_loss:0.2608 | improve:* | time:4:44:01\n",
      "step: 1200 | epoch: 1 | train_loss: 0.1903 | dev_loss:0.2736 | improve: | time:5:09:53\n",
      "step: 1300 | epoch: 1 | train_loss: 0.254 | dev_loss:0.2602 | improve:* | time:5:35:33\n",
      "step: 1400 | epoch: 1 | train_loss: 0.2834 | dev_loss:0.2174 | improve:* | time:6:01:15\n",
      "step: 1500 | epoch: 1 | train_loss: 0.3327 | dev_loss:0.211 | improve:* | time:6:26:50\n",
      "step: 1600 | epoch: 1 | train_loss: 0.3669 | dev_loss:0.2442 | improve: | time:6:52:27\n",
      "step: 1700 | epoch: 1 | train_loss: 0.1815 | dev_loss:0.2172 | improve: | time:7:18:00\n",
      "step: 1800 | epoch: 1 | train_loss: 0.3228 | dev_loss:0.2228 | improve: | time:7:43:35\n",
      "step: 1900 | epoch: 1 | train_loss: 0.2382 | dev_loss:0.2027 | improve:* | time:8:09:37\n",
      "step: 2000 | epoch: 1 | train_loss: 0.1739 | dev_loss:0.1913 | improve:* | time:8:35:14\n",
      "step: 2100 | epoch: 1 | train_loss: 0.1282 | dev_loss:0.1991 | improve: | time:9:01:03\n",
      "step: 2200 | epoch: 1 | train_loss: 0.1441 | dev_loss:0.1921 | improve: | time:9:26:38\n",
      "step: 2300 | epoch: 1 | train_loss: 0.1235 | dev_loss:0.1723 | improve:* | time:9:52:14\n",
      "step: 2400 | epoch: 1 | train_loss: 0.2926 | dev_loss:0.1806 | improve: | time:10:17:54\n",
      "step: 2500 | epoch: 1 | train_loss: 0.1638 | dev_loss:0.1789 | improve: | time:10:43:38\n",
      "step: 2600 | epoch: 1 | train_loss: 0.1105 | dev_loss:0.1689 | improve:* | time:11:09:14\n",
      "step: 2700 | epoch: 1 | train_loss: 0.1775 | dev_loss:0.1825 | improve: | time:11:35:01\n",
      "step: 2800 | epoch: 1 | train_loss: 0.1752 | dev_loss:0.1789 | improve: | time:12:01:00\n",
      "step: 2900 | epoch: 1 | train_loss: 0.1558 | dev_loss:0.1623 | improve:* | time:12:26:44\n",
      "step: 3000 | epoch: 1 | train_loss: 0.3573 | dev_loss:0.1727 | improve: | time:12:52:32\n",
      "step: 3100 | epoch: 1 | train_loss: 0.1427 | dev_loss:0.1568 | improve:* | time:13:18:08\n",
      "step: 3200 | epoch: 1 | train_loss: 0.3581 | dev_loss:0.1781 | improve: | time:13:43:31\n",
      "step: 3300 | epoch: 1 | train_loss: 0.1711 | dev_loss:0.1538 | improve:* | time:14:09:05\n",
      "step: 3400 | epoch: 1 | train_loss: 0.2162 | dev_loss:0.1522 | improve:* | time:14:34:37\n",
      "step: 3500 | epoch: 1 | train_loss: 0.1181 | dev_loss:0.1438 | improve:* | time:15:00:24\n",
      "step: 3600 | epoch: 1 | train_loss: 0.1342 | dev_loss:0.1545 | improve: | time:15:26:07\n",
      "step: 3700 | epoch: 1 | train_loss: 0.07129 | dev_loss:0.1523 | improve: | time:15:51:41\n",
      "step: 3800 | epoch: 1 | train_loss: 0.1793 | dev_loss:0.1382 | improve:* | time:16:17:24\n",
      "step: 3900 | epoch: 1 | train_loss: 0.1076 | dev_loss:0.1481 | improve: | time:16:43:06\n",
      "step: 4000 | epoch: 1 | train_loss: 0.1229 | dev_loss:0.1406 | improve: | time:17:09:04\n",
      "step: 4100 | epoch: 1 | train_loss: 0.1586 | dev_loss:0.1348 | improve:* | time:17:35:04\n",
      "step: 4200 | epoch: 1 | train_loss: 0.2314 | dev_loss:0.1355 | improve: | time:18:00:55\n",
      "step: 4300 | epoch: 1 | train_loss: 0.1581 | dev_loss:0.1333 | improve:* | time:18:26:23\n",
      "step: 4400 | epoch: 1 | train_loss: 0.1023 | dev_loss:0.1537 | improve: | time:18:51:57\n",
      "step: 4500 | epoch: 1 | train_loss: 0.1446 | dev_loss:0.1533 | improve: | time:19:17:50\n",
      "step: 4600 | epoch: 1 | train_loss: 0.1434 | dev_loss:0.1334 | improve: | time:19:43:19\n",
      "step: 4700 | epoch: 1 | train_loss: 0.1614 | dev_loss:0.1441 | improve: | time:20:09:08\n",
      "step: 4800 | epoch: 1 | train_loss: 0.2332 | dev_loss:0.1249 | improve:* | time:20:34:53\n",
      "step: 4900 | epoch: 1 | train_loss: 0.08506 | dev_loss:0.1313 | improve: | time:21:00:19\n",
      "step: 5000 | epoch: 1 | train_loss: 0.1147 | dev_loss:0.1267 | improve: | time:21:26:12\n",
      "step: 5100 | epoch: 1 | train_loss: 0.1761 | dev_loss:0.1192 | improve:* | time:21:52:06\n",
      "step: 5200 | epoch: 1 | train_loss: 0.1282 | dev_loss:0.124 | improve: | time:22:17:43\n",
      "step: 5300 | epoch: 1 | train_loss: 0.1152 | dev_loss:0.131 | improve: | time:22:43:23\n",
      "step: 5400 | epoch: 1 | train_loss: 0.1631 | dev_loss:0.1226 | improve: | time:23:08:52\n",
      "step: 5500 | epoch: 1 | train_loss: 0.07046 | dev_loss:0.1178 | improve:* | time:23:34:25\n",
      "step: 5600 | epoch: 1 | train_loss: 0.1941 | dev_loss:0.122 | improve: | time:1 day, 0:00:17\n",
      "step: 5700 | epoch: 1 | train_loss: 0.1398 | dev_loss:0.1758 | improve: | time:1 day, 0:26:15\n",
      "step: 5800 | epoch: 1 | train_loss: 0.09225 | dev_loss:0.1175 | improve:* | time:1 day, 0:51:55\n",
      "step: 5900 | epoch: 1 | train_loss: 0.1256 | dev_loss:0.1123 | improve:* | time:1 day, 1:17:24\n",
      "step: 6000 | epoch: 1 | train_loss: 0.1053 | dev_loss:0.1071 | improve:* | time:1 day, 1:43:06\n",
      "step: 6100 | epoch: 1 | train_loss: 0.05309 | dev_loss:0.1051 | improve:* | time:1 day, 2:08:52\n",
      "step: 6200 | epoch: 1 | train_loss: 0.07804 | dev_loss:0.1076 | improve: | time:1 day, 2:34:42\n",
      "step: 6300 | epoch: 1 | train_loss: 0.08719 | dev_loss:0.1089 | improve: | time:1 day, 3:00:16\n",
      "step: 6400 | epoch: 1 | train_loss: 0.1749 | dev_loss:0.1077 | improve: | time:1 day, 3:25:50\n",
      "step: 6500 | epoch: 1 | train_loss: 0.1548 | dev_loss:0.1062 | improve: | time:1 day, 3:51:23\n",
      "step: 6600 | epoch: 1 | train_loss: 0.08939 | dev_loss:0.1033 | improve:* | time:1 day, 4:17:04\n",
      "step: 6700 | epoch: 1 | train_loss: 0.06034 | dev_loss:0.105 | improve: | time:1 day, 4:42:46\n",
      "step: 6800 | epoch: 1 | train_loss: 0.1859 | dev_loss:0.09545 | improve:* | time:1 day, 5:08:34\n",
      "step: 6900 | epoch: 1 | train_loss: 0.1383 | dev_loss:0.1029 | improve: | time:1 day, 5:34:34\n",
      "step: 7000 | epoch: 1 | train_loss: 0.1228 | dev_loss:0.1023 | improve: | time:1 day, 6:00:40\n",
      "step: 7100 | epoch: 1 | train_loss: 0.1247 | dev_loss:0.1021 | improve: | time:1 day, 6:26:17\n",
      "step: 7200 | epoch: 1 | train_loss: 0.08277 | dev_loss:0.09844 | improve: | time:1 day, 6:52:00\n",
      "step: 7300 | epoch: 1 | train_loss: 0.115 | dev_loss:0.09836 | improve: | time:1 day, 7:17:58\n",
      "step: 7400 | epoch: 1 | train_loss: 0.07981 | dev_loss:0.0922 | improve:* | time:1 day, 7:43:45\n",
      "step: 7500 | epoch: 1 | train_loss: 0.09143 | dev_loss:0.1033 | improve: | time:1 day, 8:09:19\n",
      "step: 7600 | epoch: 1 | train_loss: 0.03398 | dev_loss:0.09987 | improve: | time:1 day, 8:34:57\n",
      "step: 7700 | epoch: 1 | train_loss: 0.2882 | dev_loss:0.09489 | improve: | time:1 day, 9:00:31\n",
      "step: 7800 | epoch: 1 | train_loss: 0.1043 | dev_loss:0.09657 | improve: | time:1 day, 9:26:04\n",
      "step: 7900 | epoch: 1 | train_loss: 0.05987 | dev_loss:0.09458 | improve: | time:1 day, 9:51:42\n",
      "step: 8000 | epoch: 1 | train_loss: 0.3024 | dev_loss:0.08727 | improve:* | time:1 day, 10:17:43\n",
      "step: 8100 | epoch: 1 | train_loss: 0.1693 | dev_loss:0.09059 | improve: | time:1 day, 10:43:36\n",
      "step: 8200 | epoch: 1 | train_loss: 0.1218 | dev_loss:0.09009 | improve: | time:1 day, 11:09:19\n",
      "step: 8300 | epoch: 1 | train_loss: 0.06852 | dev_loss:0.09265 | improve: | time:1 day, 11:34:54\n",
      "step: 8400 | epoch: 1 | train_loss: 0.06166 | dev_loss:0.08586 | improve:* | time:1 day, 12:00:47\n",
      "step: 8500 | epoch: 1 | train_loss: 0.2305 | dev_loss:0.09687 | improve: | time:1 day, 12:26:25\n",
      "step: 8600 | epoch: 1 | train_loss: 0.03393 | dev_loss:0.09271 | improve: | time:1 day, 12:51:57\n",
      "step: 8700 | epoch: 1 | train_loss: 0.07099 | dev_loss:0.08923 | improve: | time:1 day, 13:17:39\n",
      "step: 8800 | epoch: 1 | train_loss: 0.04688 | dev_loss:0.08565 | improve:* | time:1 day, 13:43:32\n",
      "step: 8900 | epoch: 1 | train_loss: 0.07826 | dev_loss:0.08367 | improve:* | time:1 day, 14:09:21\n",
      "step: 9000 | epoch: 1 | train_loss: 0.06554 | dev_loss:0.09474 | improve: | time:1 day, 14:35:06\n",
      "step: 9100 | epoch: 1 | train_loss: 0.05925 | dev_loss:0.08089 | improve:* | time:1 day, 15:01:17\n",
      "step: 9200 | epoch: 1 | train_loss: 0.06748 | dev_loss:0.08299 | improve: | time:1 day, 15:26:51\n",
      "step: 9300 | epoch: 1 | train_loss: 0.05093 | dev_loss:0.08846 | improve: | time:1 day, 15:52:28\n",
      "step: 9400 | epoch: 1 | train_loss: 0.03856 | dev_loss:0.08496 | improve: | time:1 day, 16:18:06\n",
      "step: 9500 | epoch: 1 | train_loss: 0.08422 | dev_loss:0.09843 | improve: | time:1 day, 16:43:42\n",
      "step: 9600 | epoch: 1 | train_loss: 0.03078 | dev_loss:0.0819 | improve: | time:1 day, 17:09:22\n",
      "step: 9700 | epoch: 1 | train_loss: 0.119 | dev_loss:0.08521 | improve: | time:1 day, 17:35:00\n",
      "step: 9800 | epoch: 1 | train_loss: 0.08119 | dev_loss:0.07984 | improve:* | time:1 day, 18:01:01\n",
      "step: 9900 | epoch: 1 | train_loss: 0.1808 | dev_loss:0.08442 | improve: | time:1 day, 18:26:39\n",
      "step: 10000 | epoch: 1 | train_loss: 0.1037 | dev_loss:0.08425 | improve: | time:1 day, 18:52:17\n",
      "step: 10100 | epoch: 1 | train_loss: 0.1766 | dev_loss:0.09106 | improve: | time:1 day, 19:17:53\n",
      "step: 10200 | epoch: 1 | train_loss: 0.1353 | dev_loss:0.08028 | improve: | time:1 day, 19:44:07\n",
      "step: 10300 | epoch: 1 | train_loss: 0.1053 | dev_loss:0.07247 | improve:* | time:1 day, 20:09:43\n",
      "step: 10400 | epoch: 1 | train_loss: 0.02884 | dev_loss:0.07117 | improve:* | time:1 day, 20:35:24\n",
      "step: 10500 | epoch: 1 | train_loss: 0.03831 | dev_loss:0.07418 | improve: | time:1 day, 21:01:05\n",
      "step: 10600 | epoch: 1 | train_loss: 0.1305 | dev_loss:0.07076 | improve:* | time:1 day, 21:26:45\n",
      "step: 10700 | epoch: 1 | train_loss: 0.07112 | dev_loss:0.07512 | improve: | time:1 day, 21:52:18\n",
      "step: 10800 | epoch: 1 | train_loss: 0.112 | dev_loss:0.07589 | improve: | time:1 day, 22:18:02\n",
      "step: 10900 | epoch: 1 | train_loss: 0.07797 | dev_loss:0.07257 | improve: | time:1 day, 22:43:47\n",
      "step: 11000 | epoch: 1 | train_loss: 0.1234 | dev_loss:0.07271 | improve: | time:1 day, 23:09:20\n",
      "step: 11100 | epoch: 1 | train_loss: 0.0782 | dev_loss:0.07226 | improve: | time:1 day, 23:35:08\n",
      "step: 11200 | epoch: 1 | train_loss: 0.03809 | dev_loss:0.07377 | improve: | time:2 days, 0:00:54\n",
      "step: 11300 | epoch: 1 | train_loss: 0.05509 | dev_loss:0.07136 | improve: | time:2 days, 0:26:30\n",
      "step: 11400 | epoch: 1 | train_loss: 0.03646 | dev_loss:0.07258 | improve: | time:2 days, 0:52:10\n",
      "step: 11500 | epoch: 1 | train_loss: 0.04226 | dev_loss:0.07878 | improve: | time:2 days, 1:18:06\n",
      "step: 11600 | epoch: 1 | train_loss: 0.109 | dev_loss:0.07688 | improve: | time:2 days, 1:43:45\n",
      "No optimization for a long time, auto-stopping...\n"
     ]
    }
   ],
   "source": [
    "model = BERT_LSTM_CRF(config)\n",
    "optimizer = optim.Adam(model.parameters(),lr=config.lr,weight_decay=config.weight_decay)\n",
    "eval_loss = 10000\n",
    "last_improve = 0\n",
    "require_improvement = 1000\n",
    "flag = False\n",
    "step = 0\n",
    "start_time = time.time()\n",
    "model.train()\n",
    "\n",
    "for epoch in range(config.base_epoch):\n",
    "    for i,batch in enumerate(train_loader):\n",
    "        step +=1\n",
    "        model.zero_grad()\n",
    "        inputs_id,masks_attention,tags_id = batch\n",
    "        # inputs_id,masks_attention,tags_id = Variable(inputs_id),Variable(masks_attention),Variable(tags_id)\n",
    "        if config.use_cuda:\n",
    "            inputs_id,masks_attention,tags_id = inputs_id.cuda(),masks_attention.cuda(),tags_id.cuda()\n",
    "        feats = model(inputs_id,masks_attention)\n",
    "        loss = model.loss(feats,masks_attention,tags_id)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if step % 100 == 0:\n",
    "            loss_temp = dev(model,dev_loader,epoch,config)\n",
    "            if loss_temp < eval_loss:\n",
    "                eval_loss = loss_temp\n",
    "                torch.save(model,config.save_path)\n",
    "                last_improve = step\n",
    "                improve = '*'\n",
    "            else:\n",
    "                improve = ''\n",
    "            time_dif = get_time_dif(start_time)\n",
    "            print('step: {} | epoch: {} | train_loss: {:5.4} | dev_loss:{:5.4} | improve:{} | time:{}'.format(step,epoch+1,loss.item(),loss_temp,improve,time_dif))\n",
    "        if step - last_improve > require_improvement:\n",
    "            # 验证集loss超过1000batch没下降，结束训练\n",
    "            print(\"No optimization for a long time, auto-stopping...\")\n",
    "            flag = True\n",
    "            break\n",
    "    if flag:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dev(model,dev_loader,epoch,config):\n",
    "    model.eval()\n",
    "    eval_loss = 0\n",
    "    true = []\n",
    "    pred = []\n",
    "    length = 0\n",
    "    with torch.no_grad():\n",
    "        for i,batch in enumerate(dev_loader):\n",
    "            inputs,masks,tags = batch\n",
    "            length += 1\n",
    "            # inputs,masks,tags = Variable(inputs),Variable(masks),Variable(tags)\n",
    "            if config.use_cuda:\n",
    "                inputs,masks,tags = inputs.cuda(),masks.cuda(),tags.cuda()\n",
    "            feats = model(inputs,masks)\n",
    "            path_score,best_path = model.crf(feats,masks.byte())\n",
    "            loss = model.loss(feats,masks,tags)\n",
    "            eval_loss += loss.item()\n",
    "        # pred.extend([t for t in  best_path])\n",
    "        # true.extend([t for t in tags])\n",
    "    # pred = flatten_lists(pred)\n",
    "    # true = flatten_lists(true)\n",
    "    # metrics = Metrics(true_list, predict_list, remove_O=False)\n",
    "    # print('eval epoch:{}| loss:{}'.format(epoch,eval_loss/length))\n",
    "    model.train()\n",
    "    return eval_loss/length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(config,test_data):\n",
    "    model = torch.load(config.save_path)\n",
    "    model.eval()\n",
    "    true = []\n",
    "    pred = []\n",
    "    with torch.no_grad():\n",
    "        for i,batch in enumerate(test_data):\n",
    "            inputs,masks,tags = batch\n",
    "            if config.use_cuda:\n",
    "                inputs,masks,tags = inputs.cuda(),masks.cuda(),tags.cuda()\n",
    "            feats = model(inputs,masks)\n",
    "            path_score,best_path = model.crf(feats,masks.byte())\n",
    "            pred.extend([t.cpu().numpy() for t in  best_path])\n",
    "            true.extend([t.cpu().numpy() for t in tags])\n",
    "    return pred,true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred,true = test(config,test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#预测数据和真实数据在[sep]以前都是相同的，但是在[sep]以后就不同了，具体原因不太清楚，不过[sep]后面都是[pad]不太重要，我们截取[sep]前面的信息就可以了，在tag2id里面，[sep]对应的是9\n",
    "pred_lists = []\n",
    "true_lists = []\n",
    "for i in range(len(pred)):\n",
    "    pred_list = []\n",
    "    true_list = []\n",
    "    for j in range(len(pred[i])):\n",
    "        if pred[i][j] != 9:\n",
    "            pred_list.append(pred[i][j])\n",
    "            true_list.append(true[i][j])\n",
    "        else:\n",
    "            pred_list.append(pred[i][j])\n",
    "            true_list.append(true[i][j])\n",
    "            break\n",
    "    pred_lists.append(pred_list)\n",
    "    true_lists.append(true_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_lists(lists):\n",
    "    flatten_list = []\n",
    "    for l in lists:\n",
    "        if type(l) == list:\n",
    "            flatten_list += l\n",
    "        else:\n",
    "            flatten_list.append(l)\n",
    "    return flatten_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_lists = flatten_lists(pred_lists)\n",
    "true_lists = flatten_lists(true_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2tag = {}\n",
    "for key,value in tag2id.items():\n",
    "    id2tag[value] = key\n",
    "\n",
    "for i in range(len(pred_lists)):\n",
    "    pred_lists[i] = id2tag[pred_lists[i]]\n",
    "    true_lists[i] = id2tag[true_lists[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "           precision    recall  f1-score   support\n",
      "    B-ORG     0.9525    0.9444    0.9484     52071\n",
      "    B-LOC     0.9826    0.9989    0.9907     11642\n",
      "        O     0.9936    0.9929    0.9932    791286\n",
      "    <eos>     1.0000    1.0000    1.0000     22159\n",
      "  <start>     1.0000    1.0000    1.0000     22159\n",
      "    I-LOC     0.9985    0.9980    0.9983      2019\n",
      "    E-LOC     0.9824    0.9987    0.9905     11642\n",
      "    I-ORG     0.9573    0.9744    0.9658     59220\n",
      "    E-ORG     0.9574    0.9493    0.9534     52071\n",
      "avg/total     0.9876    0.9876    0.9876   1024269\n",
      "\n",
      "Confusion Matrix:\n",
      "          B-ORG   B-LOC       O   <eos> <start>   I-LOC   E-LOC   I-ORG   E-ORG \n",
      "  B-ORG   49178       3    2236       0       0       0       0     654       0 \n",
      "  B-LOC       0   11629      13       0       0       0       0       0       0 \n",
      "      O    1984     201  785629       0       1       1     205    1435    1830 \n",
      "  <eos>       0       0       0   22159       0       0       0       0       0 \n",
      "<start>       0       0       0       0   22159       0       0       0       0 \n",
      "  I-LOC       0       2       2       0       0    2015       0       0       0 \n",
      "  E-LOC       1       0      12       0       0       2   11627       0       0 \n",
      "  I-ORG     458       0     686       0       0       0       3   57704     369 \n",
      "  E-ORG      10       0    2145       0       0       0       0     483   49433 \n"
     ]
    }
   ],
   "source": [
    "metrics = Metrics(true_lists, pred_lists, remove_O=False)\n",
    "metrics.report_scores()\n",
    "metrics.report_confusion_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config\n",
    "bert_word2id = json.load(open('data/bert_word2id.txt'))\n",
    "bert_tag2id = json.load(open('data/bert_tag2id.txt'))"
   ]
  },
  {
   "source": [
    "def extract_entity(sent,bert_word2id,bert_tag2id,config):\n",
    "    model = torch.load(config.save_path)\n",
    "    sent_list = []\n",
    "    for i in range(len(sent)):\n",
    "        sent_list.append(sent[i])\n",
    "    sent_list,_ = pad_data([sent_list],[[]],config.max_length)\n",
    "    input_ids = tensorized(sent_list,bert_word2id).cuda()\n",
    "    masked_data = get_mask_tensor(sent_list).cuda()\n",
    "\n",
    "    feats = model(input_ids,masked_data)\n",
    "    path_score,best_path = model.crf(feats,masked_data.byte())\n",
    "    pred_list = best_path[0].cpu().numpy()\n",
    "\n",
    "    id2tag = {}\n",
    "    for key,value in bert_tag2id.items():\n",
    "        id2tag[value] = key\n",
    "    pred_tag_lists = []\n",
    "    for i in range(len(pred_list)):\n",
    "        pred_tag_lists.append(id2tag[pred_list[i]])\n",
    "\n",
    "\n",
    "    entity_idxs = []\n",
    "    for i in range(len(pred_tag_lists)):\n",
    "        entity_idx = []\n",
    "        if pred_tag_lists[i]=='B-ORG':\n",
    "            entity_idx.append(i-1)\n",
    "            for j in range(i+1,len(pred_tag_lists)):\n",
    "                if pred_tag_lists[j]=='E-ORG':\n",
    "                    entity_idx.append(j-1)\n",
    "                    break\n",
    "        \n",
    "        if pred_tag_lists[i]=='B-LOC':\n",
    "            entity_idx.append(i-1)\n",
    "            for j in range(i+1,len(pred_tag_lists)):\n",
    "                if pred_tag_lists[j]=='E-LOC':\n",
    "                    entity_idx.append(j-1)\n",
    "                    break\n",
    "\n",
    "        if entity_idx!=[]:\n",
    "            entity_idxs.append(entity_idx)\n",
    "    \n",
    "    words_list = []\n",
    "    for pos in entity_idxs:\n",
    "        words = sent[pos[0]:(pos[1]+1)]\n",
    "        words_list.append(words)\n",
    "    return list(set(words_list))  "
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['三京', '北京', '双眼皮']"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "# sent = '我人在摩西，最近想在割双眼皮'\n",
    "sent = '我人在三京，三点式双眼皮在北京做'\n",
    "extract_entity(sent,bert_word2id,bert_tag2id,config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}